{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Open_File(file_name):\n",
    "    '''Function takes one input argument, a JSON-file,corresponding to the saved file-name,for each park, \n",
    "    from the result of the request to Twitters API and converts the JSON-file to a dictionary.   \n",
    "    IN: file_name(JSON-object).\n",
    "    OUT: Tweet_Data (Dictionary), Dictionary of the JSON-file.'''\n",
    "    \n",
    "    with open(file_name) as file:\n",
    "        Tweet_Data= json.load(file)\n",
    "\n",
    "    return Tweet_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uncomment below rows for quick visualization'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ToDataframe_text(Tweet_Data):\n",
    "    '''  Function takes a dictionary corresponding to the JSON-file for each park, Extracts the \"Text\" data and returns\n",
    "    a Pandas DataFrame with one column,Tweet/Text-data, were each row represents one unique Tweet.\n",
    "    IN: Tweet_Data(Dictionary).\n",
    "    OUT: DataFrame, with one column, Tweet/text-data.'''\n",
    "    \n",
    "    Df_list=[]\n",
    "    for item in Tweet_Data['statuses']:\n",
    "        if item['truncated']==True:\n",
    "            Df_list.append(item['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            Df_list.append(item['text'])\n",
    "    pd.set_option('display.max_colwidth',-1)\n",
    "    Df= pd.DataFrame(data=Df_list,columns=['Tweet'])\n",
    "    return Df\n",
    "\n",
    "'uncomment below rows for quick visualization'\n",
    "\n",
    "#Tweet_Data=Open_File('TwitterData_Json'+'/Hagaparken_Tweet15-19.json')\n",
    "#df_text= ToDataframe_text(Tweet_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uncomment below rows for quick visualization'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing pythons regular expression library.\n",
    "import re\n",
    "def clean_text_round1(text):\n",
    "    ''' Funtion takes one input argument,the tweet column in our Dataframe,and removes URL-references and special characters\n",
    "    that doesnt influence the sentiment such att #,numbers and /(see below) using pythons regular expression library. \n",
    "    Function returns a cleaned DataFramed. \n",
    "    IN: DataFranme, Tweet(text)-data column.\n",
    "    OUT: DataFrame, cleaned from URL-references and special characters.'''\n",
    "    \n",
    "    text=re.sub(r'((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)','',text)\n",
    "    text=re.sub(r'@\\S+ ', r'', text)\n",
    "    text= re.sub(r'[@0-9.#/\"?;():-]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "'uncomment below rows for quick visualization'\n",
    "#round1=lambda tweet:clean_text_round1(tweet)\n",
    "#df = pd.DataFrame(df_text['Tweet'].apply(round1))\n",
    "#df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "def clean_emoji(tweet):\n",
    "    \n",
    "    tweet=tweet.lower()\n",
    "    tweet= re.sub(r'[^a-ö \\s]','',tweet)\n",
    "    tweet= re.sub(r'[\\n]',' ',tweet)\n",
    "     \n",
    "    return tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uncomment below rows for quick visualization'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First filtering\n",
    "from functools import reduce\n",
    "# 'Stockholms',Kungsholmen,rålis,län,sthlm,mälarstrand\n",
    "\n",
    "def remove(tweet):\n",
    "    patternsToRemove = [\n",
    "        r'[Ss]tockholm(s)?',\n",
    "        r'[Kk]ungsholmen',\n",
    "        r'[Rr]ålis?',\n",
    "        r'[Ss]thlm',\n",
    "        r'[Ll]än',\n",
    "        r'[Mm]älarstrand',\n",
    "        r'[Kk]om(mer)?'\n",
    "    ]\n",
    "    \n",
    "    return reduce(lambda tweet, pattern: re.sub(pattern, '', tweet), patternsToRemove, tweet)\n",
    "    \n",
    "    \n",
    "'uncomment below rows for quick visualization'  \n",
    "#data_clean['Tweet']=data_clean['Tweet'].apply(lambda tweet: remove(tweet))\n",
    "#data_clean.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering common words\n",
    "#data_clean=pd.read_pickle('filter_2Sk')\n",
    "\n",
    "# filtering more words\n",
    "filter_list=['hela','år','idag','norr','runt','mer','bakom','inför','få','ena','ska','fick','vill','lite','kör',\n",
    "             'tack','går','vill']\n",
    "\n",
    "\n",
    "def filter(tweet):\n",
    "    tweet=tweet.lower()\n",
    "    tweet_split= tweet.split(' ')\n",
    "    \n",
    "    for word in filter_list:\n",
    "        if word in tweet_split:\n",
    "            tweet_split.remove(word)\n",
    "\n",
    "    \n",
    "    if ' '.join(tweet_split):\n",
    "        return ' '.join(tweet_split)\n",
    "    \n",
    "    else:\n",
    "        return 'tom'\n",
    "    \n",
    "#data_clean['Tweet']= data_clean['Tweet'].apply(lambda tweet: filter(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "import logging\n",
    "\n",
    "''' The function below runs all the previous steps and returns the variables \"id2word\",\"coprus\" which are going to be \n",
    "input variables in the lda-model in next cell. The function also returns the variable \"df_dtm\" which represents the cleaned\n",
    "dataframe.'''\n",
    "\n",
    "def data_toDtm():\n",
    "\n",
    "    # Reading Tweet-data\n",
    "    Tweet_Data = Open_File('../TwitterData_Json/Rålambshovsparken_Tweet15-19.json')\n",
    "    df_text = ToDataframe_text(Tweet_Data)\n",
    "    \n",
    "    # cleaning round 1\n",
    "    round1 = lambda tweet:clean_text_round1(tweet)\n",
    "    df = pd.DataFrame(df_text['Tweet'].apply(round1))\n",
    "\n",
    "    # removing all characters that isnt a alphabetic letter.\n",
    "    df['Tweet'] = df['Tweet'].apply(lambda tweet: clean_emoji(tweet))\n",
    "\n",
    "    \n",
    "    # Removing common expressed words.\n",
    "    df['Tweet'] = df['Tweet'].apply(lambda tweet: remove(tweet))\n",
    "    df['Tweet'] = df['Tweet'].apply(lambda tweet: filter(tweet))\n",
    " \n",
    "    # Delete these row indexes from dataFrame\n",
    "    del_rows= df[df['Tweet']=='tom'].index\n",
    "    df.drop(del_rows , inplace=True)\n",
    "    \n",
    "    \n",
    "    # In order to Implement the LDA-algoritm we Transform the DataFrame column Tweet-data to a Document Term Matrix\n",
    "    cv = CountVectorizer(max_df=0.9,min_df=0.0025) \n",
    "    data_cv = cv.fit_transform(df['Tweet'])\n",
    "    data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "    data_dtm.index = df.index\n",
    "\n",
    "    # pickle the data\n",
    "    pickle.dump(cv,open('cv.pkl_rå','wb'))\n",
    "\n",
    "    #Required input for topic modeling is a term-document matrix\n",
    "    #Which is the transpose of a DTM\n",
    "    # columns=Tweets, rows= word \n",
    "    tdm=data_dtm.transpose()\n",
    "    \n",
    "    # putting the term-document matrix into a new gensim format, sparse matrix for efficency.\n",
    "    sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "    corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "    # Gensim also requires dictionary of all the terms and their respective location in the tdm\n",
    "    cv = pickle.load(open('cv.pkl_rå','rb'))\n",
    "    id2word = dict((v,k) for k, v in cv.vocabulary_.items())\n",
    "    \n",
    "    return id2word, corpus, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theod\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "'Output from the function each lda-model takes the variables \"id2word\" and \"corpus\" as input.'\n",
    "id2word,corpus,df_dtm = data_toDtm()\n",
    "# 3 topics\n",
    "#lda= models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3,passes=50)\n",
    "#lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identified Topics\n",
    "### topic 1: Crime\n",
    "### topic 2: Holidays celebration (National day),midsummer\n",
    "### topic 3: Konserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstop_words= stopwords.words('swedish')\\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\\n\\ncloud = WordCloud(stopwords=stop_words,\\n                  background_color='white',\\n                  width=2500,\\n                  height=1800,\\n                  max_words=20,\\n                  colormap='tab10',\\n                  color_func=lambda *args, **kwargs: cols[i],\\n                  prefer_horizontal=1.0)\\n\\ntopics = lda.show_topics(num_words=20,formatted=False)\\n\\nfig, axes = plt.subplots(1, 3, figsize=(18,6), sharex=True, sharey=True)\\n\\nfor i, ax in enumerate(axes.flatten()):\\n    fig.add_subplot(ax)\\n    topic_words = dict(topics[i][1])\\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\\n    plt.gca().imshow(cloud)\\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\\n    plt.gca().axis('off')\\n\\n\\nplt.subplots_adjust(wspace=0, hspace=0)\\nplt.axis('off')\\nplt.margins(x=0, y=0)\\nplt.tight_layout()\\nplt.show()\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.colors as mcolors\n",
    "'''\n",
    "stop_words= stopwords.words('swedish')\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda.show_topics(num_words=20,formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18,6), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()'''\n",
    "\n",
    "#fig.savefig('Rålambshovsparken_wordcloud.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=df_dtm['Tweet']):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda, corpus=corpus, texts=df_dtm['Tweet'])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "#df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in lda.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from matplotlib.ticker import FuncFormatter\\nsns.set(rc={'figure.figsize':(20,10)})\\nsns.set_palette('deep')\\nfig,ax_count= plt.subplots(figsize=(12,10))\\n\\n#Countplot All Sentiment\\nax_count=sns.countplot(x='Dominant_Topic',data=df_dominant_topic)\\ntick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\\nax_count.xaxis.set_major_formatter(tick_formatter)\\n\\nax_count.set_xlabel('Topic',fontsize=18) \\nax_count.set_ylabel('Number of Tweets',fontsize=18)\\nax_count.set_title('Number of Tweets by Dominant Topic',fontsize=20)\\n\\n#fig.savefig('Rålambshovsparken_Dominanttopic.pdf')\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from matplotlib.ticker import FuncFormatter\n",
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "sns.set_palette('deep')\n",
    "fig,ax_count= plt.subplots(figsize=(12,10))\n",
    "\n",
    "#Countplot All Sentiment\n",
    "ax_count=sns.countplot(x='Dominant_Topic',data=df_dominant_topic)\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax_count.xaxis.set_major_formatter(tick_formatter)\n",
    "\n",
    "ax_count.set_xlabel('Topic',fontsize=18) \n",
    "ax_count.set_ylabel('Number of Tweets',fontsize=18)\n",
    "ax_count.set_title('Number of Tweets by Dominant Topic',fontsize=20)\n",
    "\n",
    "#fig.savefig('Rålambshovsparken_Dominanttopic.pdf')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
